<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c9{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c4{margin: 0 auto; background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c10{color:#1155cc;text-decoration:underline}.c6{color:inherit;text-decoration:inherit}.c7{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c4"><p class="c3"><span class="c5">Advanced Poisson Panoramas</span></p><p class="c3"><span class="c8">By Rishi Kapadia and Dorian Chan</span></p><p class="c3 c7"><span class="c0"></span></p><p class="c2"><span>In
 this project, we tried to go above and beyond panoramas by integrating 
Poisson techniques into stitching together multiple images. We 
implemented data structures like quadtrees and algorithms like grid 
based solvers in order to facilitate faster runtimes as well as more 
pleasing results. All photos were taken by us!</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c9">The Process</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">First, we decided to try cylindrical projections, where we repeatedly take images and project them on a unit cylinder. </span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 296.57px; height: 197.50px;"><img alt="" src="index_files/image2.jpg" style="width: 296.57px; height: 197.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 296.25px; height: 214.50px;"><img alt="" src="index_files/image8.png" style="width: 296.25px; height: 214.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">In
 order to test these projections out, we used a synthetic dataset we 
found online. We convolved the images together using a pyramid 
technique, where we downsize the images to identify good locations, and 
check around these locations at higher resolutions.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 124.00px;"><img alt="" src="index_files/image15.png" style="width: 624.00px; height: 124.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">Unfortunately,
 this breaks down for more realistic panoramas. We tried more 
complicated convolution techniques in the frequency domain and on edges,
 but that didn’t work so well.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 105.33px;"><img alt="" src="index_files/image18.png" style="width: 624.00px; height: 105.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">Thus, we decided to use more advanced feature detection. Below are the input images. </span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 301.93px; height: 201.50px;"><img alt="" src="index_files/image13.jpg" style="width: 301.93px; height: 201.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 301.93px; height: 201.50px;"><img alt="" src="index_files/image2.jpg" style="width: 301.93px; height: 201.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">First,
 we need to detect potential features in the 2 images we want to 
combine. To do this, we first find Harris corners, which are pixels that
 have a gradient magnitude greater than a threshold in both the x and y 
directions.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 300.50px; height: 202.36px;"><img alt="" src="index_files/image17.png" style="width: 336.11px; height: 230.63px; margin-left: -26.92px; margin-top: -8.36px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 302.16px; height: 202.50px;"><img alt="" src="index_files/image11.png" style="width: 337.03px; height: 229.35px; margin-left: -28.01px; margin-top: -7.23px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">We
 reduced the number of potential features from all pixels in the image 
to just the Harris corners, but there are still hundreds of potential 
features. To reduce that number further, we use non-max suppression, to 
choose the 500 Harris corners which are the most spread apart from each 
other, to get a good distribution of features around the image.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 300.50px; height: 202.40px;"><img alt="" src="index_files/image9.png" style="width: 336.80px; height: 231.38px; margin-left: -29.21px; margin-top: -8.99px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 302.50px; height: 203.41px;"><img alt="" src="index_files/image25.png" style="width: 339.34px; height: 233.43px; margin-left: -28.62px; margin-top: -9.56px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">We
 compute a feature descriptor for each of these points, which is a 20x20
 region around the pixel, and downsample the descriptors to 8x8 cells.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 248.08px; height: 310.50px;"><img alt="" src="index_files/image24.png" style="width: 248.08px; height: 310.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 248.50px; height: 311.02px;"><img alt="" src="index_files/image23.png" style="width: 248.50px; height: 311.02px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">To
 find feature correspondences between the two images, we compute the 
first 2 nearest neighbors using the feature descriptors. We keep only 
the pairs of points for which the ratio of the 1st nearest neighbor to 
the 2nd nearest neighbor is greater than a threshold. This gets rid of 
most of the points that do not correspond to any point in the other 
image.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 292.38px; height: 194.50px;"><img alt="" src="index_files/image29.png" style="width: 326.99px; height: 223.05px; margin-left: -26.81px; margin-top: -8.69px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 292.38px; height: 194.50px;"><img alt="" src="index_files/image10.png" style="width: 326.99px; height: 223.05px; margin-left: -26.81px; margin-top: -8.69px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">The
 nearest neighbor method doesn't robustly eliminate all false 
correspondences. Thus, we use the RANSAC algorithm to randomly choose 4 
pairs of point correspondences, compute the best translation offset 
using those points, and calculate how many pairs are within 2 pixels of 
that translation. We iterate over this many times and keep the pairs 
corresponding to the translation that agrees with the most pairs. 
Eliminating the faulty correspondences gives us the pairs of points that
 have similar features in both images.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 298.29px; height: 199.50px;"><img alt="" src="index_files/image19.png" style="width: 333.64px; height: 228.09px; margin-left: -26.73px; margin-top: -8.26px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 301.50px; height: 201.97px;"><img alt="" src="index_files/image1.png" style="width: 337.20px; height: 230.91px; margin-left: -27.65px; margin-top: -7.72px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">We
 then decided to work on blending the distinct images together. We 
decided to implement Poisson blending in order to perform stitching, 
which provides very nice results.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 62.67px;"><img alt="" src="index_files/image26.png" style="width: 624.00px; height: 62.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">Unfortunately,
 we found that it was very time and memory intensive. Thus, we decided 
to try and implement different techniques to minimize these drawbacks, 
and compare the results. However, we found many of these techniques were
 very susceptible to poor boundary conditions, as you can see the black 
shadows in the image below. </span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 66.67px;"><img alt="" src="index_files/image27.png" style="width: 624.00px; height: 72.59px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">We
 decided to dub our first technique Poisson pyramid blending. We first 
built a pyramid of differing resolutions of our image. For each 
resolution, we cut the image into multiple “picture frame” slices, and 
then solved the Poisson equation for each slice in isolation using the 
previous resolution for our boundary conditions. We found that this 
technique used about ¼ the memory of naive Poisson and the same amount 
of time. However, we found this technique to be particularly susceptible
 to boundary conditions, as we got a lot of washing out or shadowing. We
 had to modify the Poisson equation to also be weighted by the value of 
the source pixel in order to solve these problems, as you can see below.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 76.00px;"><img alt="" src="index_files/image22.png" style="width: 624.00px; height: 76.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">For
 our second technique, we decided to use a quadtree to reduce the 
dimensionality. The ideal output Laplacian should be the average of the 
two Laplacians in the overlapping area. Thus, we first calculate a crude
 composition and its Laplacian, and then calculate the difference 
between the ideal and the crude Laplacian. We then apply a quadtree to 
this difference, where we repeatedly subdivide nodes until each node has
 a variance below a desired threshold. We use this quadtree to reduce 
the dimensionality from 60000 to about 2000, which is very helpful for 
both memory and runtime usage. We then solve for the difference, and 
then add this output value to our crude composition in order to build 
our output. We found a couple drawbacks with this technique. First, our 
matrices were no longer square, which meant we had to use least squares 
solvers - we got inconsistent results. The two below images were run 
using the same matrices and code. The left is an example of a good 
output, while the right image is a poor output, with a very visible seam
 and color saturation.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 286.50px; height: 103.10px;"><img alt="" src="index_files/image21.png" style="width: 286.50px; height: 103.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 293.50px; height: 103.75px;"><img alt="" src="index_files/image5.png" style="width: 293.50px; height: 103.75px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">Below is an example of the phenomena applied to an entire panorama:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 94.67px;"><img alt="" src="index_files/image14.png" style="width: 624.00px; height: 94.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">As
 a fun application, we decided to try applying the above techniques to 
blending two distinct images together. We found that RANSAC and feature 
detection failed to find any correspondences between the images, so we 
had to fall back onto convolution techniques, which seemed to do 
decently, as you can see below. </span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 137.33px;"><img alt="" src="index_files/image12.png" style="width: 624.00px; height: 137.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">We
 found that mixed gradients worked well for these scenarios, while 
seamless gradients worked better for the panoramas shown above. Below is
 an example of applying mixed gradients to entire panoramas. As you can 
see, the image doesn’t look quite right.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 96.00px;"><img alt="" src="index_files/image20.png" style="width: 624.00px; height: 101.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">As
 a benchmark, we decided to also implement Laplacian Pyramid blending, 
which for the most part provided comparable results to our Poisson 
implementations.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 89.33px;"><img alt="" src="index_files/image4.png" style="width: 624.00px; height: 89.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">We
 also implemented spherical projections - however, we weren’t able to 
take or find any pictures suitable for this. Thus, we decided to just 
try them on our existing cylindrical photos.</span></p><p class="c2"><span class="c0">Cylindrical:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 113.33px;"><img alt="" src="index_files/image16.png" style="width: 1876.56px; height: 117.93px; margin-left: -701.94px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Spherical:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 117.33px;"><img alt="" src="index_files/image28.png" style="width: 1924.17px; height: 121.22px; margin-left: -701.94px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">We
 also found that existing panorama projection techniques failed to work 
for panoramas taken at an angle - i.e. looking up at the sky. We decided
 to try projecting the images onto either downward or upward facing 
cones. You can see the slight distortion below. However, we found that 
it was nigh impossible to match the photos together using feature 
detection or even convolution without a lot of distortion.</span></p><p class="c2"><span class="c0">Original:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 226.38px; height: 169.78px;"><img alt="" src="index_files/image6.jpg" style="width: 226.38px; height: 169.78px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Downwards:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 208.27px; height: 169.79px;"><img alt="" src="index_files/image3.png" style="width: 208.27px; height: 169.79px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Upwards:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 208.27px; height: 169.79px;"><img alt="" src="index_files/image7.png" style="width: 208.27px; height: 169.79px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">We also developed a viewer for 360 degree panoramas, but we can’t show that in this writeup!</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">We had a couple ideas for future work. </span></p><p class="c2"><span class="c0">It
 might be beneficial to compare “normalized” gradients relative to the 
entire image instead of raw gradients in the mixed gradients technique, 
as we found mixed gradients failed in situations with complex 
backgrounds.</span></p><p class="c2"><span class="c0">In Poisson, 
instead of using quadtrees, we could use more complicated basis 
functions in order to reduce dimensionality, such as Fourier 
decompositions or splines.</span></p><p class="c2"><span class="c0">For 
our quadtree technique, we could instead build our Laplacian matrix on 
the quadtree itself, instead of checking points from the original image.
 We would then get a square matrix, and lose a lot of the numerical 
instability.</span></p><p class="c2"><span class="c0">Our pyramid 
technique could be easily parallelized, as we cut the images into 
multiple independent slices and solve the Poisson equation 
independently.</span></p><p class="c2"><span class="c0">One repeated 
problem we came across a lot in Poisson is that boundary conditions are 
not very nice, especially when half your image hangs off the side of 
another image. Coming up with better ways instead of weighting by the 
original image would provide more seamless results and more consistent 
color across a panorama.</span></p><p class="c2"><span class="c0">In the
 scenario where we blend together distinct images, we could possibly 
modify the feature detection to work with different angles and 
rescaling, which would be helpful for finding correspondences.</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">Thanks for reading!</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span>Our Github is </span><span class="c10"><a class="c6" href="https://www.google.com/url?q=https://github.com/antiamoeba/poisson&amp;sa=D&amp;ust=1502260516629000&amp;usg=AFQjCNHrqXye0_sKWzQjawijIK-pKgIOdA">here</a></span><span class="c0">.</span></p></body></html>